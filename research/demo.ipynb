{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "\n",
    "    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "    and the end index 'end'. The 'theta' parameter scales the frequencies.\n",
    "    The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Dimension of the frequency tensor.\n",
    "        end (int): End index for precomputing frequencies.\n",
    "        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Precomputed frequency tensor with complex exponential.\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "\n",
    "    cos, sin = freqs.cos(), freqs.sin()\n",
    "\n",
    "    return torch.stack((cos, -sin, sin, cos), dim=-1).view(*freqs.size(), 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tensor = precompute_freqs_cis(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000, -0.0000],\n",
       "          [ 0.0000,  1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5403, -0.8415],\n",
       "          [ 0.8415,  0.5403]]],\n",
       "\n",
       "\n",
       "        [[[-0.4161, -0.9093],\n",
       "          [ 0.9093, -0.4161]]],\n",
       "\n",
       "\n",
       "        [[[-0.9900, -0.1411],\n",
       "          [ 0.1411, -0.9900]]],\n",
       "\n",
       "\n",
       "        [[[-0.6536,  0.7568],\n",
       "          [-0.7568, -0.6536]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2837,  0.9589],\n",
       "          [-0.9589,  0.2837]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9602,  0.2794],\n",
       "          [-0.2794,  0.9602]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7539, -0.6570],\n",
       "          [ 0.6570,  0.7539]]],\n",
       "\n",
       "\n",
       "        [[[-0.1455, -0.9894],\n",
       "          [ 0.9894, -0.1455]]],\n",
       "\n",
       "\n",
       "        [[[-0.9111, -0.4121],\n",
       "          [ 0.4121, -0.9111]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freq_cis(dim, end, theta):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    \n",
    "    cos, sin = freqs.cos(), freqs.sin()\n",
    "    return torch.stack((cos, -sin, sin, cos), dim=-1).view(*freqs.size() , 2, 2)\n",
    "\n",
    "def reshape_freq_tensor(freq_cis, x, seq_dim):\n",
    "    ndim=x.ndim\n",
    "    shape = [\n",
    "        d if i ==  seq_dim or i == ndim - 3 else 1 for i, d in enumerate(x.shape[:-2])\n",
    "    ]\n",
    "    return freq_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_embedding(\n",
    "  xq: torch.Tensor,\n",
    "  xk: torch.Tensor,\n",
    "  seq_dim: int,\n",
    "  freq_cis: torch.Tensor  \n",
    "):\n",
    "    xq_ = xq.reshape(*xq.shape[:-1], -1, 1, 2)\n",
    "    xk_ = xk.reshape(*xk.shape[:-1], -1, 1, 2)\n",
    "    freq_cis = reshape_freq_tensor(\n",
    "        freq_cis, xq_, seq_dim\n",
    "    ).float()\n",
    "    xq_out = (xq_ * freq_cis).sum(5).flatten(3)\n",
    "    xk_out = (xk_ * freq_cis).sum(5).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, theta, head_dim, max_seqlen):\n",
    "        super().__init__()\n",
    "        self.theta = theta\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seqlen = max_seqlen\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"freq_cis\",\n",
    "            precompute_freq_cis(\n",
    "                dim=self.head_dim,\n",
    "                end=self.max_seqlen,\n",
    "                theta=self.theta\n",
    "            ),\n",
    "            persistent=False\n",
    "        )\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.freq_cis[...] = precompute_freq_cis(\n",
    "            dim=self.head_dim,\n",
    "            end=self.max_seqlen,\n",
    "            theta=self.theta\n",
    "        )\n",
    "    \n",
    "    def forward(self, seqlen: Optional[int] = None, token_id: Optional[torch.Tensor] = None):\n",
    "        \n",
    "        check = seqlen is None or token_id is None\n",
    "        assert check, \"Either seqlen or token_id must be provided.\"\n",
    "        if token_id is not None:\n",
    "            return self.freq_cis[token_id]\n",
    "        elif seqlen is not None:\n",
    "            return self.freq_cis[:seqlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotary = RotaryEmbedding(\n",
    "    theta=10000,\n",
    "    head_dim=64,\n",
    "    max_seqlen=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 5.4030e-01, -8.4147e-01],\n",
       "          [ 8.4147e-01,  5.4030e-01]],\n",
       "\n",
       "         [[ 7.3176e-01, -6.8156e-01],\n",
       "          [ 6.8156e-01,  7.3176e-01]],\n",
       "\n",
       "         [[ 8.4601e-01, -5.3317e-01],\n",
       "          [ 5.3317e-01,  8.4601e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0000e+00, -2.3714e-04],\n",
       "          [ 2.3714e-04,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -1.7783e-04],\n",
       "          [ 1.7783e-04,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -1.3335e-04],\n",
       "          [ 1.3335e-04,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-4.1615e-01, -9.0930e-01],\n",
       "          [ 9.0930e-01, -4.1615e-01]],\n",
       "\n",
       "         [[ 7.0948e-02, -9.9748e-01],\n",
       "          [ 9.9748e-01,  7.0948e-02]],\n",
       "\n",
       "         [[ 4.3146e-01, -9.0213e-01],\n",
       "          [ 9.0213e-01,  4.3146e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0000e+00, -4.7427e-04],\n",
       "          [ 4.7427e-04,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -3.5566e-04],\n",
       "          [ 3.5566e-04,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -2.6670e-04],\n",
       "          [ 2.6670e-04,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-9.9984e-01, -1.7612e-02],\n",
       "          [ 1.7612e-02, -9.9984e-01]],\n",
       "\n",
       "         [[ 6.1640e-01,  7.8744e-01],\n",
       "          [-7.8744e-01,  6.1640e-01]],\n",
       "\n",
       "         [[-7.2419e-01, -6.8960e-01],\n",
       "          [ 6.8960e-01, -7.2419e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 9.7083e-01, -2.3976e-01],\n",
       "          [ 2.3976e-01,  9.7083e-01]],\n",
       "\n",
       "         [[ 9.8356e-01, -1.8057e-01],\n",
       "          [ 1.8057e-01,  9.8356e-01]],\n",
       "\n",
       "         [[ 9.9075e-01, -1.3573e-01],\n",
       "          [ 1.3573e-01,  9.9075e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.5504e-01,  8.3182e-01],\n",
       "          [-8.3182e-01, -5.5504e-01]],\n",
       "\n",
       "         [[ 9.8774e-01,  1.5612e-01],\n",
       "          [-1.5612e-01,  9.8774e-01]],\n",
       "\n",
       "         [[-9.8034e-01, -1.9732e-01],\n",
       "          [ 1.9732e-01, -9.8034e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 9.7078e-01, -2.3999e-01],\n",
       "          [ 2.3999e-01,  9.7078e-01]],\n",
       "\n",
       "         [[ 9.8353e-01, -1.8074e-01],\n",
       "          [ 1.8074e-01,  9.8353e-01]],\n",
       "\n",
       "         [[ 9.9073e-01, -1.3586e-01],\n",
       "          [ 1.3586e-01,  9.9073e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.0007e-01,  9.1649e-01],\n",
       "          [-9.1649e-01,  4.0007e-01]],\n",
       "\n",
       "         [[ 8.2917e-01, -5.5900e-01],\n",
       "          [ 5.5900e-01,  8.2917e-01]],\n",
       "\n",
       "         [[-9.3457e-01,  3.5578e-01],\n",
       "          [-3.5578e-01, -9.3457e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 9.7072e-01, -2.4022e-01],\n",
       "          [ 2.4022e-01,  9.7072e-01]],\n",
       "\n",
       "         [[ 9.8350e-01, -1.8092e-01],\n",
       "          [ 1.8092e-01,  9.8350e-01]],\n",
       "\n",
       "         [[ 9.9071e-01, -1.3600e-01],\n",
       "          [ 1.3600e-01,  9.9071e-01]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotary(\n",
    "    seqlen=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freq_cis(dim, end, theta):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    \n",
    "    cos, sin = freqs.cos(), freqs.sin()\n",
    "    return torch.stack((cos, -sin, sin, cos), dim=-1).view(*freqs.size() , 2, 2)\n",
    "\n",
    "def reshape_freq_tensor(freq_cis, x, seq_dim):\n",
    "    ndim=x.ndim\n",
    "    shape = [\n",
    "        d if i ==  seq_dim or i == ndim - 3 else 1 for i, d in enumerate(x.shape[:-2])\n",
    "    ]+[2, 2]\n",
    "    return freq_cis.view(*shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor, seq_dim: int):\n",
    "    \"\"\"\n",
    "    Reshape frequency tensor for broadcasting it with another tensor.\n",
    "\n",
    "    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "    for the purpose of broadcasting the frequency tensor during element-wise operations.\n",
    "\n",
    "    Args:\n",
    "        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n",
    "        x (torch.Tensor): Target tensor for broadcasting compatibility.\n",
    "        seq_dim (int): Sequence dimension index.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reshaped frequency tensor.\n",
    "    \"\"\"\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= seq_dim < ndim\n",
    "    assert freqs_cis.shape == (\n",
    "        x.shape[seq_dim],\n",
    "        x.shape[-3],\n",
    "        2,\n",
    "        2,\n",
    "    ), f\"freqs_cis vs x: {(freqs_cis.shape, x.shape)}\"\n",
    "    shape = [\n",
    "        d if i == seq_dim or i == ndim - 3 else 1 for i, d in enumerate(x.shape[:-2])\n",
    "    ] + [2, 2]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embedding(\n",
    "  xq: torch.Tensor,\n",
    "  xk: torch.Tensor,\n",
    "  seq_dim: int,\n",
    "  freq_cis: torch.Tensor  \n",
    "):\n",
    "    xq_ = xq.reshape(*xq.shape[:-1], -1, 1, 2)\n",
    "    xk_ = xk.reshape(*xk.shape[:-1], -1, 1, 2)\n",
    "    freq_cis = reshape_freq_tensor(\n",
    "        freq_cis, xq_, seq_dim\n",
    "    ).float()\n",
    "    xq_out = (xq_ * freq_cis).sum(5).flatten(3)\n",
    "    xk_out = (xk_ * freq_cis).sum(5).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "frq = precompute_freq_cis(64, 4096, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4096, 12, 64)\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = x.reshape(\n",
    "    *x.shape[:-1], -1, 1, 2\n",
    ")\n",
    "new_y = x.reshape(\n",
    "    *x.shape[:-1], -1, 1, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_cis = reshape_for_broadcast(\n",
    "    frq, new_x, 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "newx_out = (new_x * freq_cis).sum(5).flatten(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4096, 12, 32, 1, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4096, 12, 32, 2, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newx_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.randn(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4172, -0.2354,  0.3734, -1.0634, -2.0294,  0.5437, -1.0728, -0.8370,\n",
       "          0.1716,  0.6862]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4172, -0.2354,  0.3734, -1.0634, -2.0294,  0.5437, -1.0728, -0.8370,\n",
       "          0.1716,  0.6862]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(scores):\n",
    "    \"\"\"\n",
    "    scores: [bs, seq_len, vocab]\n",
    "    returns [bs, seq_len]\n",
    "\n",
    "    Computes the entropy for each token in the batch.\n",
    "    Note: uses natural log.\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(scores, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "    p_log_p = log_probs * probs\n",
    "    entropy = -p_log_p.sum(dim=-1)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5168])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5168])\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "entropy = Categorical(logits=p).entropy()\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, trunc_seqlen = 4, 384\n",
    "patchstart = torch.full(\n",
    "    size=(bs, trunc_seqlen),\n",
    "    fill_value=trunc_seqlen,\n",
    "    dtype=torch.long,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[384, 384, 384,  ..., 384, 384, 384],\n",
       "        [384, 384, 384,  ..., 384, 384, 384],\n",
       "        [384, 384, 384,  ..., 384, 384, 384],\n",
       "        [384, 384, 384,  ..., 384, 384, 384]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchstart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
